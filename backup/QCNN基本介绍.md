# 1.变分量子算法

 变分量子算法是一种适合“近期有噪声中等规模量子线路”（Noisy Intermediate-Scale Quantum, NISQ）的基于变分优化的量子经典混合算法。其是在近期最有希望达成量子优势的算法方向。通过充分利用大量含噪声的量子比特的表征能力（提取特征的能力），来实现无完整纠错情况下相关问题的近似和解决。

变分量子算法典型的工作流类似于深度学习，主要区别是深度学习中的神经网络组件被参数化的量子线路替代。所谓参数化量子线路 $U(\theta)$ 指的是对应线路矩阵由可变参数组 $\theta$ 决定的线路。实现上，一般以旋转角度可变的参数化量子门来表达参数化量子线路，旋转门上的参数构成了参数集。我们通过经典的优化器来调整线路上的参数，使得参数化量子线路输出的波函数为 $|\psi\rangle=U(\theta)|0\rangle$，其对应的具体问题损失函数 $L(|\psi\rangle)$ 最小。

<div align="center">

![Image](https://github.com/user-attachments/assets/f872b12a-c04d-4369-95e0-3aac0b72fb47)

*变分量子算法结构*
</div>

## 参数化量子电路

**参数化量子电路** (PQCs)，常被称为 **Ansätze**（源自德语，意为“方法”或“设置”），构成变分量子算法的核心。正如本章引言所述，变分量子算法使用经典优化器来调整量子电路的参数。PQC 正是这种可调整的量子电路。在QCNN中，PQC构成纠缠层（全连接层）。

作用机制
它的作用是将初始状态（通常通过编码经典数据 $\vec{x}$ 准备）转换为最终状态 $|\psi(\vec{x}, \vec{\theta})\rangle$ 。对该最终状态执行测量，随后用于计算成本函数 $C(\vec{\theta})$ ，经典优化器通过调整参数 $\vec{\theta}$ 来使该函数最小化。

数学表达
形式上，PQC $U(\vec{\theta})$ 是一种幺正操作，由一系列量子门组成，其中一些门依赖于可调整的经典参数 $\vec{\theta} = (\theta_{1}, \theta_{2}, \ldots, \theta_{M})$ 。通常，机器学习中变分量子算法的整体电路形式如下：

```math
|\psi(\vec{x}, \vec{\theta})\rangle = U(\vec{\theta}) U_{\text{encode}}(\vec{x}) |0\rangle^{\otimes n}
```

这里：
*   $U_{\text{encode}}(\vec{x})$ 是数据编码电路，它将经典输入数据 $\vec{x}$ 映射到初始量子状态，通常从 $n$ 量子比特的 $|0\rangle^{\otimes n}$ 状态开始。
*   PQC $U(\vec{\theta})$ 随后进一步处理此状态。

参数与设计
$U(\vec{\theta})$ 的设计是影响变分量子算法性能的一个重要因素，包含表达能力、纠缠能力、可训练性、硬件效率等多项设计考量。
<div align="center">

![Image](https://github.com/user-attachments/assets/0d4688e6-d84e-4142-b41e-bccbbab89a1e)

*四种基本纠缠层*
</div>

(a)Brick Wall 结构中，门以交错的方式排列，形成砖墙的结构。每层门连接相邻的两个量子比特，层间交替连接未直接相连的比特，确保纠缠均匀分布。

(b) Lambda 结构中，门的排列呈现出类似字母“Λ”的形状。多个 CNOT 门在一层中同时作用，但连接顺序较为复杂，适合实现更高阶的纠缠。

(c) Chain 结构中，所有量子比特通过链式连接逐一作用。每个量子比特仅与其直接相邻的比特进行 CNOT 操作，形成线性结构，适合简单的数据传递。

(d) Star Connection 结构中，一个中心量子比特与所有其他量子比特直接相连，形成星形结构。这种设计集中化信息处理，适合快速分发或收集信息。

四种不同门的排列顺序和连接方式带来的是置乱能力的不同，置乱能力指的是 QNN 在信息处理过程中对输入信息的混淆和重组能力。具体而言，置乱能力是通过操作符大小的增长来表征的，这种增长反映了在量子线路中信息如何从读取量子比特扩散到所有输入量子比特，且 QNN 的学习效率与其置乱能力密切相关。在训练过程中，若 QNN 能够有效地将信息从输出量子比特传递到输入量子比特，则其提取信息的效率也会提高。其中用得最多的是Chain结构，因为它效果较好。

# 2.QCNN基本结构

## 2.1 QCNN分类

在[量子机器学习](https://en.wikipedia.org/wiki/Quantum_machine_learning)中，按照数据类型和算法类型的不同可以将它们简单分为四类（经典-量子的排列组合）

<div align="center">

![Image](https://github.com/user-attachments/assets/50cedf6a-ccf3-46e1-9808-3fd82409bf71)

*量子机器学习分类*
</div>

其中Q-C是量子数据来源，经典计算机分析，多用于化学物理的量子分析；而Q-Q使用全量子数据和计算，需要量子计算机才能实现；我们讨论的范畴主要是C-Q即数据集经典，算法量子化的量子机器学习（当然我们的实践也仅仅是在算法上模拟量子计算机，实际整个架构还在经典计算机上运行）。

## 2.2 QCNN基本结构

经典卷积神经网络是传统机器学习中最常见的一种模型，常用于图像处理任务，用于提取特征相关任务。其主要特点是卷积神经网络主要关注的是局部信息的特征，而不是全局数据。以图像输入为例，卷积神经网络通过设定的卷积核依次对图像中的局部区域进行处理，处理后的结果构成了新的对象，一般称之为特征图，随后可以由后续层进行下一步的处理。

同样的思想可以应用到量子计算当中，量子卷积神经网络就是经典卷积神经网络的量子对应物，其实现的方法一般有两种：

- 一种是基于数据嵌入的量子卷积神经网络，其思路更类似于经典卷积神经网络，通过从图像中提取局部区域，利用参数化旋转门将其编码为量子态，在通过参数化量子线路实现特征映射，最后通过量子测量获取不同通道的输出特征，但并不包含卷积神经网络中的池化操作，需要用经典池化操作来完成。其具体实现流程如下图所示。

<div align="center">

![Image](https://github.com/user-attachments/assets/1835fa9f-5bfd-4dd8-bdf8-fb8290c0dd23)

*基于数据嵌入的量子卷积神经网络*
</div>

- 另一种是基于层次量子池化的卷积神经网络，其在网络架构上更类似于深度神经网络。该方法通过参数化量子变化层和量子池化层逐渐减少量子比特数，完成局部特征信息的提取。该网络架构将经典卷积网络中的卷积层和池化层结合在一起，可以在不需要经典网络的帮助下，单独实现一个卷积操作。其具体实现流程如下图所示。

<div align="center">

![Image](https://github.com/user-attachments/assets/5ee8da42-17b2-4168-a08a-7e2aba7b89e0)

*QCNN的基本流程框架*
</div>

# 3.编码层

## 3.1 维度挑战

编码向量 $x=(x_{1},x_{2},...,x_{N})$ 最直接、信息无损的方式似乎是振幅编码。在这里， $N=2^{n}$ 个特征被归一化并编码为 n 个量子比特量子态的振幅：

```math
|\phi(x)\rangle=\frac{1}{\|x\|}\sum_{i=1}^{N}x_{i}|i\rangle
```

这里 $|i\rangle$ 代表与 i 的二进制表示对应的计算基态。这实现了指数级压缩，仅使用 n 个量子比特编码了 $2^{n}$ 个特征。然而，制备像 $|\phi(x)\rangle$ 这样的任意量子态通常很困难。已知算法通常需要电路深度与 n 成多项式甚至指数级增长，可能抵消所寻求的量子优势。此外，从结果状态中提取有关特定特征（振幅）的信息并非易事。

基编码中，每个特征可能控制一个特定量子比特或一组量子比特的状态，它随特征数量 (N) 线性扩展（N 个特征需要 $\mathcal{O}(N)$ 个量子比特），使其在当前硬件上不适合高维数据。

角度编码将特征 $x_{i}$ 映射到应用于单个量子比特的旋转角度 $\theta_{i}(x_{i})$ （例如， $R_{X}(\theta_{i}(x_{i}))$  、 $R_{Y}(\theta_{i}(x_{i}))$  、 $R_{Z}(\theta_{i}(x_{i}))$ ），如果每个特征都有自己的旋转门，通常需要 $n\approx N$ 个量子比特。尽管存在使用更少量子比特但电路更复杂的变体（例如密集角度编码或数据重上传），但基本的难题依然存在：我们如何有效地压缩高维信息？

## 3.2 高维数据编码策略

面对这些限制，采用了一些策略，通常是**组合使用**：

### 经典降维

在考虑量子编码之前，应用经典降维技术通常是最实用的第一步。如果数据的固有维度远低于环境维度N，可以采用以下方法：

- 主成分分析(PCA)：找到数据中最大方差的正交方向。将数据投影到前k个主成分上，可以在显著降低维度的同时保留大部分方差。
- 自动编码器：训练用于将数据压缩成低维潜在表示(编码)然后重建(解码)的神经网络。压缩后的潜在向量可以随后编码到量子比特上。
- 特征选择：识别并选择原始特征中最具信息量的子集的算法，基于统计属性或模型性能。

目标是获得一个低维表示 $x' \in \mathbb{R}^{N'}$ ，其中 $N' \ll N$ ，这样更容易使用量子方法编码到 $n \approx N'$ 或更少量子比特上。这种预处理步骤利用强大的经典机器学习工具，并将量子资源集中在最重要的信息上。

### 量子比特高效的量子编码方案

即使经过经典降维，N'对于朴素编码可能仍然太大。量子比特高效的量子编码策略旨在利用n < N'个量子比特：

- 振幅编码（再审视）：尽管其状态制备存在挑战，但如果 $N' = 2^{n}$ ，它仍然是最量子比特高效的方法。针对与机器学习任务相关的特定状态类别的有效制备方法正在进行研究。如果后续量子算法能够整体处理振幅中的信息而无需单独访问它们，则尤其重要。
- 角度编码变体：
  - 密集角度编码：多个特征(k)可以通过围绕不同轴的旋转序列编码到单个量子比特上，例如 $R_{Z}(\theta_{1})R_{Y}(\theta_{2})R_{Z}(\theta_{3})\dots$ ，并且 $\theta_{j}$ 取决于特征的某个子集。如果每个量子比特编码k个特征，N'个特征大约需要 $n \approx N'/k$ 个量子比特。
  - 数据重上传：采用数据重上传的电路可以使用固定且可能较少量的量子比特(n)处理高维数据。经典数据x影响整个电路中的多个参数化门。这隐式地创建了一个复杂的特征映射，而无需N'个量子比特。表达能力取决于电路深度和结构，而不仅仅是量子比特的数量。

<div align="center">

![Image](https://github.com/user-attachments/assets/98f7e90f-ffdf-4ca1-862a-0d2068fb6f9a)

*数据重上传/增量数据上传（Incremental Data-Uploading，IDU）*
</div>

---

#### 数据重上传的益处与挑战

数据重上传的主要益处在于其增加表达能力的潜力。这意味着模型可能能够：

- 表示更复杂的函数或决策边界。

- 相较于尝试学习相同功能的单次上传模型，可能以更少的量子位达到良好性能。

- 创建在经典计算中可能难以复制的有效特征映射。

然而，也存在重要的设计选择和潜在挑战：

- **电路设计**：多少个重上传层L最优？U(x)和W(θi)应包含哪些特定门？这些选择严重依赖于具体问题和可用的量子硬件。没有放之四海而皆准的答案。

- **可训练性**：尽管表达能力增加，训练的复杂度也可能增加。深度重上传电路可能更容易受到优化困难的影响，例如贫瘠高原（我们将在第4章详细讨论）。梯度可能消失，使得训练非常缓慢或不可能。

- **过拟合**：高度表达能力模型有时可能过拟合训练数据，无法泛化到新的、未见的数据。正则化技术可能变得有必要。

- **硬件资源**：每个重上传步骤都会增加电路深度，从而增加计算时间以及当前量子设备对噪声的敏感性。设计硬件高效的U(x)和W(θi)层非常重要。

---

### 随机投影

受约翰逊-林登施特劳斯引理等经典技术启发，随机投影可以将高维数据映射到较低维空间，同时近似保持距离。量子对应方法涉及应用由数据参数化的随机（或伪随机）量子电路。尽管在电路深度方面可能高效，但分析此类随机特征映射的特性是一个活跃的研究方向（就是映射关系不好分析，毕竟是随机生成的，有点玄学的意思）。

# 4.卷积层和池化层

经典卷积神经网络（CNN）在处理图像识别等网格状数据结构的任务中表现出色。它们通过应用卷积滤波器捕获局部模式的层以及在保留重要特征的同时降低维度的池化层来实现这一目的，从而形成输入的层次化表示。量子卷积神经网络（QCNN）的目标是将这些成功方法应用于量子环境，运用量子电路进行类似卷积和池化的操作。

主要原因在于处理量子态或编码为量子态的经典数据，这可能发现经典CNN无法获取的特征或关联。QCNN尤其适合处理具有固有空间结构的问题，例如分析多体系统量子模拟的数据或某些经典图像处理任务。

## 4.1 QCNN的核心组成部分

QCNN通常包含与经典对应物相似的量子层：

- **量子卷积层**： 这一层在输入量子比特上局部施加参数化量子电路（通常是少量子比特酉算符）。可以设想一个小型量子电路在输入寄存器上“滑动”，如同经典滤波核。这些电路，即“量子滤波器”，通常是参数化的，并在不同空间位置共享相同参数，如同经典CNN中的权重共享。其作用是获取局部特征，并可能在相邻量子比特间产生纠缠。

- **量子池化层**： 卷积之后，池化层减少表示数据的量子比特数量，从而降低空间维度。一种常见方法是测量特定量子比特（通常是卷积步骤中纠缠的辅助量子比特或用于池化的量子比特），并根据测量结果应用后续的受控酉操作。这个过程能有效选择并保留特征，同时减小系统规模。与经典池化（例如最大池化）不同，量子池化由于测量而可能具有固有的概率性，并且可以根据结果进一步改变状态。

这些层通常呈**分层排列**，一个池化层的输出量子比特作为下一个卷积层的输入。

## 4.2 构建量子卷积和池化层

**量子卷积**

设输入状态由N个量子比特表示，它们可能线性排列或在2D网格上。量子卷积层对k个相邻量子比特的一小部分施加参数化酉算符U(θ)(例如k=2或k=3)。这个酉算符在输入寄存器上重复应用，可能带步长，类似于经典卷积。

例如，作用于量子比特i和i+1：
```math
|\psi_{out}\rangle = \dots U_{i+2,i+3}(\theta) \dots U_{i,i+1}(\theta) \dots |\psi_{in}\rangle
```
参数θ在U的所有应用中共享，这大大减少了参数数量，与作用于所有量子比特的通用PQC相比。U(θ)的选择决定了滤波器；它可能是一种硬件高效的拟设或旨在产生特定关联的电路。

<div align="center">

![Image](https://github.com/user-attachments/assets/47c67121-cae1-4b71-a724-6a2fddbb088c)

*密集角度编码+卷积示意图*
</div>

**量子池化**

池化操作会减少量子比特的数量。一个简单的方案可以是：根据相邻量子比特的测量结果应用受控酉操作。例如，对于量子比特i和i+1，我们可以施加一个CNOT门使它们纠缠，测量量子比特i+1，然后将其舍弃。量子比特i的状态现在可能根据i和i+1之间共享的属性而被改变，且量子比特总数也减少了。

更复杂的池化操作也存在，有时会涉及多个量子比特、测量以及受经典控制的量子门。核心在于在减少量子比特数量的同时，保留剩余量子态中编码的相关特征。

<div align="center">

![Image](https://github.com/user-attachments/assets/c23dd8ad-ea59-484b-81b6-5c40b18f1086)

*池化线路示意图*
</div>

一个完整的QCNN架构通常是堆叠多个卷积层和池化层。最终层的输出可以直接测量以获得用于分类的经典结果，或者传递给另一个量子电路（如全连接层PQC），甚至是一个经典神经网络进行最终处理。

分层结构和参数共享使得QCNN与通用PQC相比，可能更能扩展到更多输入量子比特，尽管在深层架构中仍可能出现贫瘠高原等难题。卷积酉算符U(θ)的选择和池化策略是影响网络表达能力和可训练性的重要设计环节。

# 5.全连接层

**这个层和池化层一样，并不是必须的**，通过前面提到的参数化量子电路(PQCs/Ansätze)或者叫变分量子电路实现，并利用损失和梯度函数进行优化参数。在这层之后进行测量得到经典数据输入后续的CNN经典全连接层。

- 前面提过，有仅把卷积核量子化的模型存在，也有把全连接层简化为随机的量子电路的模型，这类模型的量子化部分是静态的。或者可以说是部分量子化的。[一个利用随机全连接层实现QCNN的例子](https://deepquantum.turingq.com/2024/05/23/%E9%87%8F%E5%AD%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/)

- 全连接层可以通过 $R_Y$ 或者 $R_Z R_Y R_Z$ 两种结构实现（后者参数更多），具体单层电路如下：
<div align="center">

![Image](https://github.com/user-attachments/assets/732a0127-12eb-40ee-b8cb-e660868fa067)

*量子全连接层网络的两种量子线路实现*

</div>

整体来看是这样的：

<div align="center">

![Image](https://github.com/user-attachments/assets/32a324fb-757c-46f4-8cea-162d763bca63)

*单卷积层的QCNN量子线路*
</div>


# 6.损失&梯度选择