# 1.变分量子算法

 变分量子算法是一种适合“近期有噪声中等规模量子线路”（Noisy Intermediate-Scale Quantum, NISQ）的基于变分优化的量子经典混合算法。其是在近期最有希望达成量子优势的算法方向。通过充分利用大量含噪声的量子比特的表征能力（提取特征的能力），来实现无完整纠错情况下相关问题的近似和解决。

变分量子算法典型的工作流类似于深度学习，主要区别是深度学习中的神经网络组件被参数化的量子线路替代。所谓参数化量子线路 $U(\theta)$ 指的是对应线路矩阵由可变参数组 $\theta$ 决定的线路。实现上，一般以旋转角度可变的参数化量子门来表达参数化量子线路，旋转门上的参数构成了参数集。我们通过经典的优化器来调整线路上的参数，使得参数化量子线路输出的波函数为 $|\psi\rangle=U(\theta)|0\rangle$，其对应的具体问题损失函数 $L(|\psi\rangle)$ 最小。

<div align="center">

![Image](https://github.com/user-attachments/assets/f872b12a-c04d-4369-95e0-3aac0b72fb47)

*变分量子算法结构*
</div>

## 参数化量子电路

**参数化量子电路** (PQCs)，常被称为 **Ansätze**（源自德语，意为“方法”或“设置”），构成变分量子算法的核心。正如本章引言所述，变分量子算法使用经典优化器来调整量子电路的参数。PQC 正是这种可调整的量子电路。在QCNN中，PQC构成纠缠层（全连接层）。

作用机制
它的作用是将初始状态（通常通过编码经典数据 $\vec{x}$ 准备）转换为最终状态 $|\psi(\vec{x}, \vec{\theta})\rangle$ 。对该最终状态执行测量，随后用于计算成本函数 $C(\vec{\theta})$ ，经典优化器通过调整参数 $\vec{\theta}$ 来使该函数最小化。

数学表达
形式上，PQC $U(\vec{\theta})$ 是一种幺正操作，由一系列量子门组成，其中一些门依赖于可调整的经典参数 $\vec{\theta} = (\theta_{1}, \theta_{2}, \ldots, \theta_{M})$ 。通常，机器学习中变分量子算法的整体电路形式如下：

```math
|\psi(\vec{x}, \vec{\theta})\rangle = U(\vec{\theta}) U_{\text{encode}}(\vec{x}) |0\rangle^{\otimes n}
```

这里：
*   $U_{\text{encode}}(\vec{x})$ 是数据编码电路，它将经典输入数据 $\vec{x}$ 映射到初始量子状态，通常从 $n$ 量子比特的 $|0\rangle^{\otimes n}$ 状态开始。
*   PQC $U(\vec{\theta})$ 随后进一步处理此状态。

参数与设计
$U(\vec{\theta})$ 的设计是影响变分量子算法性能的一个重要因素，包含表达能力、纠缠能力、可训练性、硬件效率等多项设计考量。
<div align="center">

![Image](https://github.com/user-attachments/assets/0d4688e6-d84e-4142-b41e-bccbbab89a1e)

*四种基本纠缠层*
</div>

(a)Brick Wall 结构中，门以交错的方式排列，形成砖墙的结构。每层门连接相邻的两个量子比特，层间交替连接未直接相连的比特，确保纠缠均匀分布。

(b) Lambda 结构中，门的排列呈现出类似字母“Λ”的形状。多个 CNOT 门在一层中同时作用，但连接顺序较为复杂，适合实现更高阶的纠缠。

(c) Chain 结构中，所有量子比特通过链式连接逐一作用。每个量子比特仅与其直接相邻的比特进行 CNOT 操作，形成线性结构，适合简单的数据传递。

(d) Star Connection 结构中，一个中心量子比特与所有其他量子比特直接相连，形成星形结构。这种设计集中化信息处理，适合快速分发或收集信息。

四种不同门的排列顺序和连接方式带来的是置乱能力的不同，置乱能力指的是 QNN 在信息处理过程中对输入信息的混淆和重组能力。具体而言，置乱能力是通过操作符大小的增长来表征的，这种增长反映了在量子线路中信息如何从读取量子比特扩散到所有输入量子比特，且 QNN 的学习效率与其置乱能力密切相关。在训练过程中，若 QNN 能够有效地将信息从输出量子比特传递到输入量子比特，则其提取信息的效率也会提高。其中用得最多的是Chain结构，因为它效果较好。

# 2.QCNN基本结构

## 2.1 QCNN分类

在[量子机器学习](https://en.wikipedia.org/wiki/Quantum_machine_learning)中，按照数据类型和算法类型的不同可以将它们简单分为四类（经典-量子的排列组合）

<div align="center">

![Image](https://github.com/user-attachments/assets/50cedf6a-ccf3-46e1-9808-3fd82409bf71)

*量子机器学习分类*
</div>

其中Q-C是量子数据来源，经典计算机分析，多用于化学物理的量子分析；而Q-Q使用全量子数据和计算，需要量子计算机才能实现；我们讨论的范畴主要是C-Q即数据集经典，算法量子化的量子机器学习（当然我们的实践也仅仅是在算法上模拟量子计算机，实际整个架构还在经典计算机上运行）。

## 2.2 QCNN基本结构

经典卷积神经网络是传统机器学习中最常见的一种模型，常用于图像处理任务，用于提取特征相关任务。其主要特点是卷积神经网络主要关注的是局部信息的特征，而不是全局数据。以图像输入为例，卷积神经网络通过设定的卷积核依次对图像中的局部区域进行处理，处理后的结果构成了新的对象，一般称之为特征图，随后可以由后续层进行下一步的处理。

同样的思想可以应用到量子计算当中，量子卷积神经网络就是经典卷积神经网络的量子对应物，其实现的方法一般有两种：

- 一种是基于数据嵌入的量子卷积神经网络，其思路更类似于经典卷积神经网络，通过从图像中提取局部区域，利用参数化旋转门将其编码为量子态，在通过参数化量子线路实现特征映射，最后通过量子测量获取不同通道的输出特征，但并不包含卷积神经网络中的池化操作，需要用经典池化操作来完成。其具体实现流程如下图所示。

<div align="center">

![Image](https://github.com/user-attachments/assets/1835fa9f-5bfd-4dd8-bdf8-fb8290c0dd23)

*基于数据嵌入的量子卷积神经网络*
</div>

- 另一种是基于层次量子池化的卷积神经网络，其在网络架构上更类似于深度神经网络。该方法通过参数化量子变化层和量子池化层逐渐减少量子比特数，完成局部特征信息的提取。该网络架构将经典卷积网络中的卷积层和池化层结合在一起，可以在不需要经典网络的帮助下，单独实现一个卷积操作。其具体实现流程如下图所示。

<div align="center">

![Image](https://github.com/user-attachments/assets/5ee8da42-17b2-4168-a08a-7e2aba7b89e0)

*QCNN的基本流程框架*
</div>

# 3.编码层

## 3.1 维度挑战

编码向量 $x=(x_{1},x_{2},...,x_{N})$ 最直接、信息无损的方式似乎是振幅编码。在这里， $N=2^{n}$ 个特征被归一化并编码为 n 个量子比特量子态的振幅：

```math
|\phi(x)\rangle=\frac{1}{\|x\|}\sum_{i=1}^{N}x_{i}|i\rangle
```

这里 $|i\rangle$ 代表与 i 的二进制表示对应的计算基态。这实现了指数级压缩，仅使用 n 个量子比特编码了 $2^{n}$ 个特征。然而，制备像 $|\phi(x)\rangle$ 这样的任意量子态通常很困难。已知算法通常需要电路深度与 n 成多项式甚至指数级增长，可能抵消所寻求的量子优势。此外，从结果状态中提取有关特定特征（振幅）的信息并非易事。

基编码中，每个特征可能控制一个特定量子比特或一组量子比特的状态，它随特征数量 (N) 线性扩展（N 个特征需要 $\mathcal{O}(N)$ 个量子比特），使其在当前硬件上不适合高维数据。

角度编码将特征 $x_{i}$ 映射到应用于单个量子比特的旋转角度 $\theta_{i}(x_{i})$ （例如， $R_{X}(\theta_{i}(x_{i}))$  、 $R_{Y}(\theta_{i}(x_{i}))$  、 $R_{Z}(\theta_{i}(x_{i}))$ ），如果每个特征都有自己的旋转门，通常需要 $n\approx N$ 个量子比特。尽管存在使用更少量子比特但电路更复杂的变体（例如密集角度编码或数据重上传），但基本的难题依然存在：我们如何有效地压缩高维信息？

## 3.2 高维数据编码策略

面对这些限制，采用了一些策略，通常是**组合使用**：

### 经典降维

在考虑量子编码之前，应用经典降维技术通常是最实用的第一步。如果数据的固有维度远低于环境维度N，可以采用以下方法：

- 主成分分析(PCA)：找到数据中最大方差的正交方向。将数据投影到前k个主成分上，可以在显著降低维度的同时保留大部分方差。
- 自动编码器：训练用于将数据压缩成低维潜在表示(编码)然后重建(解码)的神经网络。压缩后的潜在向量可以随后编码到量子比特上。
- 特征选择：识别并选择原始特征中最具信息量的子集的算法，基于统计属性或模型性能。

目标是获得一个低维表示 $x' \in \mathbb{R}^{N'}$ ，其中 $N' \ll N$ ，这样更容易使用量子方法编码到 $n \approx N'$ 或更少量子比特上。这种预处理步骤利用强大的经典机器学习工具，并将量子资源集中在最重要的信息上。

### 量子比特高效的量子编码方案

即使经过经典降维，N'对于朴素编码可能仍然太大。量子比特高效的量子编码策略旨在利用n < N'个量子比特：

- 振幅编码（再审视）：尽管其状态制备存在挑战，但如果 $N' = 2^{n}$ ，它仍然是最量子比特高效的方法。针对与机器学习任务相关的特定状态类别的有效制备方法正在进行研究。如果后续量子算法能够整体处理振幅中的信息而无需单独访问它们，则尤其重要。
- 角度编码变体：
  - 密集角度编码：多个特征(k)可以通过围绕不同轴的旋转序列编码到单个量子比特上，例如 $R_{Z}(\theta_{1})R_{Y}(\theta_{2})R_{Z}(\theta_{3})\dots$ ，并且 $\theta_{j}$ 取决于特征的某个子集。如果每个量子比特编码k个特征，N'个特征大约需要 $n \approx N'/k$ 个量子比特。
  - 数据重上传：采用数据重上传的电路可以使用固定且可能较少量的量子比特(n)处理高维数据。经典数据x影响整个电路中的多个参数化门。这隐式地创建了一个复杂的特征映射，而无需N'个量子比特。表达能力取决于电路深度和结构，而不仅仅是量子比特的数量。

<div align="center">

![Image](https://github.com/user-attachments/assets/98f7e90f-ffdf-4ca1-862a-0d2068fb6f9a)

*数据重上传/增量数据上传（Incremental Data-Uploading，IDU）*
</div>

---

#### 数据重上传的益处与挑战

数据重上传的主要益处在于其增加表达能力的潜力。这意味着模型可能能够：

- 表示更复杂的函数或决策边界。

- 相较于尝试学习相同功能的单次上传模型，可能以更少的量子位达到良好性能。

- 创建在经典计算中可能难以复制的有效特征映射。

然而，也存在重要的设计选择和潜在挑战：

- **电路设计**：多少个重上传层L最优？U(x)和W(θi)应包含哪些特定门？这些选择严重依赖于具体问题和可用的量子硬件。没有放之四海而皆准的答案。

- **可训练性**：尽管表达能力增加，训练的复杂度也可能增加。深度重上传电路可能更容易受到优化困难的影响，例如贫瘠高原（我们将在第4章详细讨论）。梯度可能消失，使得训练非常缓慢或不可能。

- **过拟合**：高度表达能力模型有时可能过拟合训练数据，无法泛化到新的、未见的数据。正则化技术可能变得有必要。

- **硬件资源**：每个重上传步骤都会增加电路深度，从而增加计算时间以及当前量子设备对噪声的敏感性。设计硬件高效的U(x)和W(θi)层非常重要。

---

### 随机投影

受约翰逊-林登施特劳斯引理等经典技术启发，随机投影可以将高维数据映射到较低维空间，同时近似保持距离。量子对应方法涉及应用由数据参数化的随机（或伪随机）量子电路。尽管在电路深度方面可能高效，但分析此类随机特征映射的特性是一个活跃的研究方向（就是映射关系不好分析，毕竟是随机生成的，有点玄学的意思）。

# 4.卷积层和池化层

经典卷积神经网络（CNN）在处理图像识别等网格状数据结构的任务中表现出色。它们通过应用卷积滤波器捕获局部模式的层以及在保留重要特征的同时降低维度的池化层来实现这一目的，从而形成输入的层次化表示。量子卷积神经网络（QCNN）的目标是将这些成功方法应用于量子环境，运用量子电路进行类似卷积和池化的操作。

主要原因在于处理量子态或编码为量子态的经典数据，这可能发现经典CNN无法获取的特征或关联。QCNN尤其适合处理具有固有空间结构的问题，例如分析多体系统量子模拟的数据或某些经典图像处理任务。

## 4.1 QCNN的核心组成部分

QCNN通常包含与经典对应物相似的量子层：

- **量子卷积层**： 这一层在输入量子比特上局部施加参数化量子电路（通常是少量子比特酉算符）。可以设想一个小型量子电路在输入寄存器上“滑动”，如同经典滤波核。这些电路，即“量子滤波器”，通常是参数化的，并在不同空间位置共享相同参数，如同经典CNN中的权重共享。其作用是获取局部特征，并可能在相邻量子比特间产生纠缠。

- **量子池化层**： 卷积之后，池化层减少表示数据的量子比特数量，从而降低空间维度。一种常见方法是测量特定量子比特（通常是卷积步骤中纠缠的辅助量子比特或用于池化的量子比特），并根据测量结果应用后续的受控酉操作。这个过程能有效选择并保留特征，同时减小系统规模。与经典池化（例如最大池化）不同，量子池化由于测量而可能具有固有的概率性，并且可以根据结果进一步改变状态。

这些层通常呈**分层排列**，一个池化层的输出量子比特作为下一个卷积层的输入。

## 4.2 构建量子卷积和池化层

**量子卷积**

设输入状态由N个量子比特表示，它们可能线性排列或在2D网格上。量子卷积层对k个相邻量子比特的一小部分施加参数化酉算符U(θ)(例如k=2或k=3)。这个酉算符在输入寄存器上重复应用，可能带步长，类似于经典卷积。

例如，作用于量子比特i和i+1：
```math
|\psi_{out}\rangle = \dots U_{i+2,i+3}(\theta) \dots U_{i,i+1}(\theta) \dots |\psi_{in}\rangle
```
参数θ在U的所有应用中共享，这大大减少了参数数量，与作用于所有量子比特的通用PQC相比。U(θ)的选择决定了滤波器；它可能是一种硬件高效的拟设或旨在产生特定关联的电路。

<div align="center">

![Image](https://github.com/user-attachments/assets/47c67121-cae1-4b71-a724-6a2fddbb088c)

*密集角度编码+卷积示意图*
</div>

**量子池化**

池化操作会减少量子比特的数量。一个简单的方案可以是：根据相邻量子比特的测量结果应用受控酉操作。例如，对于量子比特i和i+1，我们可以施加一个CNOT门使它们纠缠，测量量子比特i+1，然后将其舍弃。量子比特i的状态现在可能根据i和i+1之间共享的属性而被改变，且量子比特总数也减少了。

更复杂的池化操作也存在，有时会涉及多个量子比特、测量以及受经典控制的量子门。核心在于在减少量子比特数量的同时，保留剩余量子态中编码的相关特征。

<div align="center">

![Image](https://github.com/user-attachments/assets/c23dd8ad-ea59-484b-81b6-5c40b18f1086)

*池化线路示意图*
</div>

一个完整的QCNN架构通常是堆叠多个卷积层和池化层。最终层的输出可以直接测量以获得用于分类的经典结果，或者传递给另一个量子电路（如全连接层PQC），甚至是一个经典神经网络进行最终处理。

分层结构和参数共享使得QCNN与通用PQC相比，可能更能扩展到更多输入量子比特，尽管在深层架构中仍可能出现贫瘠高原等难题。卷积酉算符U(θ)的选择和池化策略是影响网络表达能力和可训练性的重要设计环节。

# 5.全连接层

**这个层和池化层一样，并不是必须的**，通过前面提到的参数化量子电路(PQCs/Ansätze)或者叫变分量子电路实现，并利用损失和梯度函数进行优化参数。在这层之后进行测量得到经典数据输入后续的CNN经典全连接层。

- 前面提过，有仅把卷积核量子化的模型存在，也有把全连接层简化为随机的量子电路的模型，这类模型的量子化部分是静态的。或者可以说是部分量子化的。[一个利用随机全连接层实现QCNN的例子](https://deepquantum.turingq.com/2024/05/23/%E9%87%8F%E5%AD%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/)

- 全连接层可以通过 $R_Y$ 或者 $R_Z R_Y R_Z$ 两种结构实现（后者参数更多），具体单层电路如下：
<div align="center">

![Image](https://github.com/user-attachments/assets/732a0127-12eb-40ee-b8cb-e660868fa067)

*量子全连接层网络的两种量子线路实现*

</div>

整体来看是这样的：

<div align="center">

![Image](https://github.com/user-attachments/assets/32a324fb-757c-46f4-8cea-162d763bca63)

*单卷积层的QCNN量子线路*
</div>


# 6.损失函数

## 6.1 期望和损失函数

在变分量子算法(VQA)框架中，参数化量子电路(PQC)准备量子态 $|\psi(\boldsymbol{\theta})\rangle$ ，其中 $\boldsymbol{\theta}$ 表示我们希望优化的经典参数。然而，为了引导这种优化，我们需要一种方法来衡量量子态 $|\psi(\boldsymbol{\theta})\rangle$ 达到目标机器学习目的的程度。这就是代价函数C( $\boldsymbol{\theta}$ )的作用。它充当量子处理单元(QPU)和经典优化程序之间的重要连接，将量子测量结果转换为表示性能的标量值。

### 量子测量的期望

VQA评估步骤的核心在于量子测量。通常，在量子态 $|\psi(\boldsymbol{\theta})\rangle$ 准备好之后，我们会以特定的基（通常是计算基，Z-基）测量一个或多个量子比特。这个测量过程本质上是概率性的，会产生经典比特串作为结果。

为了形成适合优化的可微分代价函数，我们通常不直接使用这些比特串的原始概率。相反，我们计算所选可观测量的期望值，这是一个厄米算子 $\hat{O}$ 。对于由PQC使用参数 $\boldsymbol{\theta}$ 准备的给定量子态 $|\psi(\boldsymbol{\theta})\rangle$ ，期望值计算如下：

```math
\langle\hat{O}\rangle_{\boldsymbol{\theta}}=\langle\psi(\boldsymbol{\theta})|\hat{O}|\psi(\boldsymbol{\theta})\rangle
```

这个期望值 $\langle\hat{O}\rangle_{\boldsymbol{\theta}}$ 提供一个平滑的实数值输出，它取决于电路参数 $\boldsymbol{\theta}$ 。它表示如果我们多次测量量子态 $|\psi(\boldsymbol{\theta})\rangle$ 上的可观测物 $\hat{O}$ 将会得到的平均值。这个期望值，或从中及目标数据标签导出的函数，构成我们代价函数C( $\boldsymbol{\theta}$ )的根本。

整个过程可以如下图所示：
<div align="center">

![Image](https://github.com/user-attachments/assets/d724d4e5-2aee-4c1b-b0c2-461d4a74eea1)

*PQC的参数更新过程*
</div>

### 基于期望为不同任务设计代价函数

$C(\boldsymbol{\theta})$ 的具体形式在很大程度上取决于当前的机器学习任务。让我们查看一些常见情况：

#### 分类

在分类任务中，目标是将输入数据 $x_{i}$ 分配到几个离散类别中的一个。

- **二分类**：对于具有两个类别（例如，标记为 +1 和 -1）的问题，一种常见方法是设计 PQC 和可观测物 $\hat{O}$ ，使得期望值 $\langle\hat{O}\rangle_{\boldsymbol{\theta}}^{(i)}$ （为输入 $x_{i}$ 计算）与类别标签 $y_{i}$ 相关。例如，我们可以在第一个量子比特上测量泡利 Z 算子 $\hat{Z}_{0}$ ，目标是对于一个类别使其约为 +1，对于另一个类别约为 -1。

- **均方误差（MSE）**：一个直接的代价函数是预测期望值与目标标签之间的 MSE：

```math
C_{\text{MSE}}(\boldsymbol{\theta})=\frac{1}{N}\sum_{i=1}^{N}(\langle\hat{O}\rangle_{\boldsymbol{\theta}}^{(i)}-y_{i})^{2} 
```

其中 N 是训练样本的数量。尽管简单，但 MSE 会二次方地惩罚偏差，这对于分类而言可能并非总是最有效的损失。

- **合页损失（Hinge Loss）**：受到支持向量机（SVM）的启发，合页损失鼓励期望值具有正确的符号并高于某个裕度：

```math
C_{\text{Hinge}}(\boldsymbol{\theta})=\frac{1}{N}\sum_{i=1}^{N}\max(0,1-y_{i}\langle\hat{O}\rangle_{\boldsymbol{\theta}}^{(i)})
```

如果预测 $\langle\hat{O}\rangle_{\boldsymbol{\theta}}^{(i)}$ 具有正确的符号（ $y_{i}\langle\hat{O}\rangle_{\boldsymbol{\theta}}^{(i)}\geq 1$ ），则此损失为零，否则线性增加。

- **交叉熵损失**：这种损失在经典机器学习中常用于概率分类器。如果我们将测量结果解释为概率，则可以进行调整。例如，如果我们测量量子比特 0 并估计测量 $\vert 0\rangle$ （与标签 $y_{i}=1$ 相关联）的概率 $p(+1\vert\boldsymbol{\theta}, x_{i})$ 和测量 $\vert 1\rangle$ （与标签 $y_{i}=0$ 相关联）的概率 $p(-1\vert\boldsymbol{\theta}, x_{i})$ ，则二元交叉熵为：

```math
C_{\text{XEnt}}(\boldsymbol{\theta})=-\frac{1}{N}\sum_{i=1}^{N}\left[y_{i}\log p(+1\vert\boldsymbol{\theta}, x_{i})+(1-y_{i})\log p(-1\vert\boldsymbol{\theta}, x_{i})\right] 
```

使用交叉熵需要从测量计数中仔细估计概率，这可能是样本密集型的。它通常需要将二元标签 $\{0,1\}$ 或 $\{-1,+1\}$ 适当映射到从特定测量结果得出的概率。

- **多分类**： 将这些想法扩展到两个以上的类别通常涉及使用多个输出量子比特、不同的测量策略（例如，测量多个泡利算子）或组合二分类器。代价函数需要相应调整，通常使用 MSE 或交叉熵的多类别版本。

#### 回归

在回归中，目标是为输入 $x_{i}$ 预测一个连续值 $y_{i}$ 。

- 期望值 $\langle\hat{O}\rangle_{\boldsymbol{\theta}}^{(i)}$ 本身可以作为预测的连续输出。可观测物 $\hat{O}$ 的比例和范围理想情况下应与目标值 $y_{i}$ 的预期范围匹配，否则输出需要重新缩放。
- **均方误差（MSE）**：这是 VQA 中回归最常用的代价函数：

```math
C_{\text{MSE}}(\boldsymbol{\theta})=\frac{1}{N} \sum_{i=1}^{N} \left(\langle\hat{O}\rangle_{\boldsymbol{\theta}}^{(i)} - y_{i}\right)^{2}
```
  它直接惩罚量子模型预测与真实连续值之间的平方差。

#### 生成模型

用于生成任务的代价函数，例如使用量子电路玻恩机（QCBM）学习概率分布或训练量子生成对抗网络（QGAN），有所不同。它们通常涉及比较量子电路生成的概率分布与目标数据分布的度量（例如，最大平均差异、库尔巴克-莱布勒散度估计）或源自判别器网络的对抗性损失,感兴趣可以另行了解。


## 6.2 QCNN中PQC的损失函数选择

### 损失函数=期望？

> ❓：目标如果是用QCNN完成二分类任务，这里的损失（代价）函数应该是刚刚提到的均方误差（MSE）吗？

**并不是**

训练变分量子算法（VQA）高度依赖于经典优化循环。如我们所论，VQA包含一个参数化量子电路（PQC） $U(\theta)$ 和一个代价函数 $C(\theta)$ ，该函数通常定义为在PQC准备的输出态上测量的可观测量 $H$ 的**期望值**:

```math
C(\theta)=\langle\psi(\theta)|H|\psi(\theta)\rangle=\langle 0|U^{\dagger}(\theta)HU(\theta)|0\rangle 
```

在变分量子算法（VQA）中，我们的核心目标不是拟合 “经典数据”，而是**求解量子系统本身的极值问题**（例如基态能量、最优策略等）。

当 VQA 被用于量子机器学习（如分类、回归）时，代价函数确实会包含经典数据的误差项（比如MSE），这里的量子机器学习指的是测量即结果的那种纯纯的量子神经网络，后续不再接入经典模型。

但是回顾一下前面我们提到的QCNN模型，其中量子部分更像是一个巨大的卷积核，虽然有类似神经网络的结构，但其实仅仅是把经典数据特征映射为高维的量子特征再进行一系列变换来映射到我们的测量轴上，而后测量回到经典CNN。这部分PQC**并非直接进行分类任务**，而是完成某种高维映射后的量子核计算，这种计算至少在原则上可能是经典不可解的，所以在这里我们的损失对象是测量期望而非数据误差。

### QCNN训练梯度传递

为了更好地说明PQC的训练损失选用期望的原因，以及各组件的作用，这里展示一下QCNN中的梯度传递，其中详细的梯度计算与梯度下降将在下一篇中介绍。

#### 1. 前向传播（数据流向）

1. **经典输入**：图像或向量数据 `x`
2. **经典预处理层**：提取低级特征 $f_{\text{classical}}(x)$
3. **量子数据编码**：将经典特征映射为量子态 $|\psi_{\text{enc}}(x)\rangle$
4. **量子卷积层（QConv）**：提取量子特征 $|\psi_{\text{conv}}(x,\theta_{\text{conv}})\rangle$
5. **量子全连接层（PQC）**：高维特征映射 $|\psi_{\text{fc}}(x,\theta_{\text{fc}})\rangle$
6. **量子测量**：得到期望值 $\langle\hat{O}\rangle=\langle\psi_{\text{fc}}|\hat{O}|\psi_{\text{fc}}\rangle$
7. **经典分类头**：输出预测概率 $\hat{y}=f_{\text{head}}(\langle\hat{O}\rangle)$

#### 2. 损失计算

- 经典损失函数（如交叉熵）：
```math
\mathcal{L}=\mathbb{E}\left[\text{CrossEntropy}(\hat{y}, y_{\text{true}})\right]
```

#### 3. 反向传播（梯度流向）

1. **经典分类头梯度**：
```math
\frac{\partial \mathcal{L}}{\partial \langle\hat{O}\rangle}
```

2. **量子全连接层（PQC）梯度**：
   - **用参数位移规则计算**：
```math
\frac{\partial\langle\hat{O}\rangle}{\partial \theta_{\mathrm{fc}, j}}=\frac{1}{2}\left[ C\left(\theta_{\mathrm{fc}}+\frac{\pi}{2} e_{j}\right)-C\left(\theta_{\mathrm{fc}}-\frac{\pi}{2} e_{j}\right)\right]
```
   - **链式法则传递**：
```math
\frac{\partial \mathcal{L}}{\partial \theta_{\mathrm{fc}, j}}=\frac{\partial \mathcal{L}}{\partial\langle\hat{O}\rangle} \cdot \frac{\partial\langle\hat{O}\rangle}{\partial \theta_{\mathrm{fc}, j}}
```
3. **量子卷积层梯度**：
   - 对卷积层参数 $\theta_{\mathrm{conv}}$ 重复类似梯度计算

4. **经典预处理层梯度**：
   - 标准反向传播更新经典层权重

#### 4. 参数更新

- 经典优化器（如 Adam）使用所有梯度更新：
  - 量子层参数 $\theta_{\mathrm{fc}}$ 和 $\theta_{\mathrm{conv}}$
  - 经典层权重