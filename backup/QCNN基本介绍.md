# 变分量子算法

 变分量子算法是一种适合“近期有噪声中等规模量子线路”（Noisy Intermediate-Scale Quantum, NISQ）的基于变分优化的量子经典混合算法。其是在近期最有希望达成量子优势的算法方向。通过充分利用大量含噪声的量子比特的表征能力（提取特征的能力），来实现无完整纠错情况下相关问题的近似和解决。

变分量子算法典型的工作流类似于深度学习，主要区别是深度学习中的神经网络组件被参数化的量子线路替代。所谓参数化量子线路 $U(\theta)$ 指的是对应线路矩阵由可变参数组 $\theta$ 决定的线路。实现上，一般以旋转角度可变的参数化量子门来表达参数化量子线路，旋转门上的参数构成了参数集。我们通过经典的优化器来调整线路上的参数，使得参数化量子线路输出的波函数为 $|\psi\rangle=U(\theta)|0\rangle$，其对应的具体问题损失函数 $L(|\psi\rangle)$ 最小。

<div align="center">

![Image](https://github.com/user-attachments/assets/f872b12a-c04d-4369-95e0-3aac0b72fb47)

*变分量子算法结构*
</div>

## 参数化量子电路

**参数化量子电路** (PQCs)，常被称为 **Ansätze**（源自德语，意为“方法”或“设置”），构成变分量子算法的核心。正如本章引言所述，变分量子算法使用经典优化器来调整量子电路的参数。PQC 正是这种可调整的量子电路。在QCNN中，PQC构成纠缠层（全连接层）。

作用机制
它的作用是将初始状态（通常通过编码经典数据 $\vec{x}$ 准备）转换为最终状态 $|\psi(\vec{x}, \vec{\theta})\rangle$ 。对该最终状态执行测量，随后用于计算成本函数 $C(\vec{\theta})$ ，经典优化器通过调整参数 $\vec{\theta}$ 来使该函数最小化。

数学表达
形式上，PQC $U(\vec{\theta})$ 是一种幺正操作，由一系列量子门组成，其中一些门依赖于可调整的经典参数 $\vec{\theta} = (\theta_{1}, \theta_{2}, \ldots, \theta_{M})$ 。通常，机器学习中变分量子算法的整体电路形式如下：

```math
|\psi(\vec{x}, \vec{\theta})\rangle = U(\vec{\theta}) U_{\text{encode}}(\vec{x}) |0\rangle^{\otimes n}
```

这里：
*   $U_{\text{encode}}(\vec{x})$ 是数据编码电路，它将经典输入数据 $\vec{x}$ 映射到初始量子状态，通常从 $n$ 量子比特的 $|0\rangle^{\otimes n}$ 状态开始。
*   PQC $U(\vec{\theta})$ 随后进一步处理此状态。

参数与设计
$U(\vec{\theta})$ 的设计是影响变分量子算法性能的一个重要因素，包含表达能力、纠缠能力、可训练性、硬件效率等多项设计考量。
<div align="center">

![Image](https://github.com/user-attachments/assets/0d4688e6-d84e-4142-b41e-bccbbab89a1e)

*四种基本纠缠层*
</div>

(a)Brick Wall 结构中，门以交错的方式排列，形成砖墙的结构。每层门连接相邻的两个量子比特，层间交替连接未直接相连的比特，确保纠缠均匀分布。

(b) Lambda 结构中，门的排列呈现出类似字母“Λ”的形状。多个 CNOT 门在一层中同时作用，但连接顺序较为复杂，适合实现更高阶的纠缠。

(c) Chain 结构中，所有量子比特通过链式连接逐一作用。每个量子比特仅与其直接相邻的比特进行 CNOT 操作，形成线性结构，适合简单的数据传递。

(d) Star Connection 结构中，一个中心量子比特与所有其他量子比特直接相连，形成星形结构。这种设计集中化信息处理，适合快速分发或收集信息。

四种不同门的排列顺序和连接方式带来的是置乱能力的不同，置乱能力指的是 QNN 在信息处理过程中对输入信息的混淆和重组能力。具体而言，置乱能力是通过操作符大小的增长来表征的，这种增长反映了在量子线路中信息如何从读取量子比特扩散到所有输入量子比特，且 QNN 的学习效率与其置乱能力密切相关。在训练过程中，若 QNN 能够有效地将信息从输出量子比特传递到输入量子比特，则其提取信息的效率也会提高。其中用得最多的是Chain结构，因为它效果较好。

# QCNN基本结构

## QCNN分类

在[量子机器学习](https://en.wikipedia.org/wiki/Quantum_machine_learning)中，按照数据类型和算法类型的不同可以将它们简单分为四类（经典-量子的排列组合）

<div align="center">

![Image](https://github.com/user-attachments/assets/50cedf6a-ccf3-46e1-9808-3fd82409bf71)

*量子机器学习分类*
</div>

其中Q-C是量子数据来源，经典计算机分析，多用于化学物理的量子分析；而Q-Q使用全量子数据和计算，需要量子计算机才能实现；我们讨论的范畴主要是C-Q即数据集经典，算法量子化的量子机器学习（当然我们的实践也仅仅是在算法上模拟量子计算机，实际整个架构还在经典计算机上运行）。

## QCNN基本结构

经典卷积神经网络是传统机器学习中最常见的一种模型，常用于图像处理任务，用于提取特征相关任务。其主要特点是卷积神经网络主要关注的是局部信息的特征，而不是全局数据。以图像输入为例，卷积神经网络通过设定的卷积核依次对图像中的局部区域进行处理，处理后的结果构成了新的对象，一般称之为特征图，随后可以由后续层进行下一步的处理。

同样的思想可以应用到量子计算当中，量子卷积神经网络就是经典卷积神经网络的量子对应物，其实现的方法一般有两种：

- 一种是基于数据嵌入的量子卷积神经网络，其思路更类似于经典卷积神经网络，通过从图像中提取局部区域，利用参数化旋转门将其编码为量子态，在通过参数化量子线路实现特征映射，最后通过量子测量获取不同通道的输出特征，但并不包含卷积神经网络中的池化操作，需要用经典池化操作来完成。其具体实现流程如下图所示。

<div align="center">

![Image](https://github.com/user-attachments/assets/1835fa9f-5bfd-4dd8-bdf8-fb8290c0dd23)

*基于数据嵌入的量子卷积神经网络*
</div>

- 另一种是基于层次量子池化的卷积神经网络，其在网络架构上更类似于深度神经网络。该方法通过参数化量子变化层和量子池化层逐渐减少量子比特数，完成局部特征信息的提取。该网络架构将经典卷积网络中的卷积层和池化层结合在一起，可以在不需要经典网络的帮助下，单独实现一个卷积操作。其具体实现流程如下图所示。

<div align="center">

![Image](https://github.com/user-attachments/assets/5ee8da42-17b2-4168-a08a-7e2aba7b89e0)

*QCNN的基本流程框架*
</div>

# 编码层

# 卷积层
# 池化层
# 全连接层
# 损失&梯度选择