本文记录我认为有意义的或者可能以后会忘掉的PyTorch小知识

# 存储冲突

这里收集一些可能会引起冲突的数据改变操作。比如你改了某个数据，但它可能会连带着其他共享同一片内存的其他相关变量一起改变了而不自知。它是内存优化带来的弊端，这些错误往往出现在一些非常基本的函数中，但我们没有细究过通过这些函数改变某个值意味着什么。

## 用Numpy导致的

NumPy 数组和 PyTorch 张量可以方便转换，但其中有冲突的隐患。 

NumPy → PyTorch

使用 $\color{red}{torch.from_numpy()}$ 时，生成的 PyTorch 张量和原始 NumPy 数组在 CPU 上共享相同的底层内存位置。这意味着修改一个对象会影响另一个。这种行为很高效，因为它避免了数据复制，但你需要注意这一点。