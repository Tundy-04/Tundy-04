# 1.梯度计算方法

训练变分量子算法（VQA）高度依赖于经典优化循环。如我们所论，VQA包含一个参数化量子电路（PQC） $U(\theta)$ 和一个成本函数 $C(\theta)$ ，该函数通常定义为在PQC准备的输出态上测量的可观测量 $H$ 的期望值：

```math
C(\theta)=\langle\psi(\theta)|H|\psi(\theta)\rangle=\langle 0|U^{\dagger}(\theta)HU(\theta)|0\rangle
```

这里的 $|0\rangle$ 是指PQC的输入态，不限于 $|0\rangle$ 。

为了使用基于梯度的办法（例如梯度下降及其变体）最小化此成本函数，我们需要计算梯度向量 $\nabla_{\theta}C(\theta)$ ，其分量为相对于电路中的每个参数 $\theta_{j}$ 的偏导数 $\frac{\partial C}{\partial \theta_{j}}$ 。由于成本函数评估涉及运行量子电路和执行测量，因此计算这些梯度需要专门为这种混合量子-经典配置设计的技术。我们来分析实践中使用的主要办法。

## 1.1 数值有限差分

最直接的办法直接借鉴于经典数值办法：有限差分。为了估算相对于参数 $\theta_{j}$ 的偏导数，我们可以在轻微扰动的参数值处评估成本函数。中心差分公式常用：

```math
\frac{\partial C}{\partial \theta_{j}} \approx \frac{C(\theta+\epsilon e_{j})-C(\theta-\epsilon e_{j})}{2\epsilon}
```

这里， $e_{j}$ 是一个在第j个位置为1、其余位置为0的单位向量， $\epsilon$ 是一个小步长。此办法需要对量子电路进行两次评估（并估算期望值），以计算每个参数 $\theta_{j}$ 的偏导数。

尽管有限差分易于理解和实现，但在量子计算背景下，它存在显著缺点：

1. $\epsilon$ 的选择：选择合适的 $\epsilon$ 比较困难。如果 $\epsilon$ 过大，近似就不准确（截断误差）。如果 $\epsilon$ 过小， $C(\theta+\epsilon e_{j})-C(\theta-\epsilon e_{j})$ 的差值可能被统计噪声（散粒噪声）所主导，这种噪声源于从有限次测量中估算期望值，导致梯度估算中出现较大误差（减法误差）。

2. 统计噪声：分子中的减法会放大散粒噪声的影响，尤其当 $C(\theta+\epsilon e_{j})$ 和 $C(\theta-\epsilon e_{j})$ 非常接近时。为了达到足够的精度，每次评估通常需要大量的测量（shots），从而增加了计算成本。

由于这些局限，尽管有限差分对于快速检查或简单问题有益，但对于训练VQA，通常倾向于使用更专门的办法。

## 1.2 参数位移规则

一种更广为采用的计算 PQC 梯度的办法是参数位移规则。对于特定类别的参数化门，该规则提供了梯度的解析表达式，避免了与在有限差分中选择  $\epsilon$ 相关的数值不稳定性。

考虑 PQC 中形式为 $G_{j}(\theta_{j})=e^{-i\frac{\theta_{j}}{2}P_{j}}$ 的一个门，其中 $P_{j}$ 是一个满足 $P_{j}^{2}=I$ 的算符。这包含常见的单比特旋转门，例如 $R_{X}(\theta_{j})=e^{-i\frac{\theta_{j}}{2}X}$ 、 $R_{Y}(\theta_{j})=e^{-i\frac{\theta_{j}}{2}Y}$ 和 $R_{Z}(\theta_{j})=e^{-i\frac{\theta_{j}}{2}Z}$ ，因为 $X^{2}=Y^{2}=Z^{2}=I$ 。

如果整个 PQC $U(\theta)$ 可以写为 $U(\theta)=VG_{j}(\theta_{j})W$ ，其中 V 和 W 是与 $\theta_{j}$ 无关的其他量子电路，那么期望值 $C(\theta)=\langle 0|U^{\dagger}(\theta)HU(\theta)|0\rangle$ 相对于 $\theta_{j}$ 的导数可以使用以下公式精确计算：
```math
\frac{\partial C(\theta)}{\partial \theta_{j}}=\frac{1}{2}\left[C\left(\theta+\frac{\pi}{2} e_{j}\right)-C\left(\theta-\frac{\pi}{2} e_{j}\right)\right]
```
这个显著的结论表明，精确导数与在参数 $\theta_{j}$ 向前和向后位移特定量 $s=\pi/2$ 后评估的成本函数差值成比例。

其原理何在？我们概述一下思路。求导涉及对门 $G_{j}(\theta_{j})$ 进行求导： $\frac{\partial G_{j}(\theta_{j})}{\partial \theta_{j}}=-\frac{i}{2} P_{j} e^{-i\frac{\theta_{j}}{2} P_{j}}=-\frac{i}{2} P_{j} G_{j}(\theta_{j})$ 。将其代入 $C(\theta)$ 的导数表达式会导致包含 $P_{j}$ 的项。主要的认识是，对于 $P_{j}^{2}=I$ 的门，算符 $P_{j}$ 可以与原始门 $G_{j}(\theta_{j})$ 的位移版本相关联。具体而言，可以证明：
```math
P_{j} G_{j}(\theta_{j})=\frac{i}{2}\left[G_{j}\left(\theta_{j}+\frac{\pi}{2}\right)-G_{j}\left(\theta_{j}-\frac{\pi}{2}\right)\right]
```
将其代回 $C(\theta)$ 的导数表达式并简化，最终得到参数位移规则公式。

<div align="center">

![Image](https://github.com/user-attachments/assets/6d47b99e-c9c7-4d79-830b-4ea287ac18a7)

*参数位移规则*
</div>

### 优点、成本与适用性

**优点：**
* **解析精确：** 它提供了真实梯度，而非数值近似（在评估 $C$ 时会受到散粒噪声影响）。
* **无需调整步长：** 它避免了选择小 $\epsilon$ 的问题。位移量 $s=\pi/2$ 是固定的。
* **韧性：** 相较于有限差分，它对散粒噪声通常更具韧性，因为位移量较大，使得差值 $C(\theta+se_{j})-C(\theta-se_{j})$ 相对于噪声基底通常更大。

**成本：**
与中心有限差分类似，参数位移规则对每个参数 $\theta_{j}$ 需要两次电路评估。

**适用性：**
基本规则直接适用于由泡利算符（X, Y, Z）生成的门。QML 中常用的大多数 PQC 拟设主要是由这类门构建的，使参数位移规则得以广泛应用。

# 2.经典优化方法

## 2.1 基于梯度的优化器

这些优化器依赖于获取成本函数关于电路参数的梯度 $\nabla_\theta C(\theta)$ 。如前一节（“梯度计算方法”）所述，参数位移规则等技术允许在量子硬件上进行解析梯度计算，尽管这会以额外的电路评估为代价。

### 随机梯度下降（SGD）变体

普通SGD根据来自一小批数据（或在最小化量子电路期望值的情形下甚至单个数据点）的梯度估算来更新参数。虽然简单，但它可能存在收敛速度慢和振荡的问题。更进阶的变体引入了动量或自适应学习率。

Adam（自适应矩估计）：Adam是经典深度学习中广泛使用的优化器，并且通常作为变分量子算法的稳固基准。它通过存储过去平方梯度的指数衰减平均值（类似RMSProp）和过去梯度的指数衰减平均值（类似动量）来计算每个参数的自适应学习率。

更新规则涉及计算有偏一阶矩 $(m_t)$ 和二阶矩 $(v_t)$ 的估算值：

```math
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t 
```

```math
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
```
其中 $g_t$ 是第t步的梯度， $\beta_1, \beta_2$ 是超参数（通常接近1，例如0.9和0.999）。计算偏差校正的估算值 $\hat{m}_t$ 和 $\hat{v}_t$ ，并更新参数：
```math
\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
```
这里， $\eta$ 是学习率， $\epsilon$是一个用于数值稳定的小常数。

- 优点：通常比普通SGD收敛更快，相对容易实现和调整（默认超参数通常效果良好）。
- 缺点：可能对量子测量中固有的噪声梯度估算敏感。有时可能收敛到次优解或表现出不稳定性，特别是在变分量子算法中常见的非凸函数曲面。与无梯度方法相比，需要计算梯度，从而增加了电路评估的次数。

### 拟牛顿法（例如，BFGS）

BFGS（Broyden-Fletcher-Goldfarb-Shanno）等方法属于拟牛顿法系列。它们旨在仅使用在连续迭代中收集的梯度信息来近似逆海森矩阵（二阶导数矩阵）。这使得它们能够估算损失的曲率并采取更明智的步长，可能导致更快的收敛，特别是在局部最小值附近。

更新方向计算为 $p_{k}=-B_{k}g_{k}$ ，其中 $B_{k}$ 是第k次迭代时逆海森矩阵的近似值。通常沿着此方向执行线搜索，以找到合适的步长。

- 优点：在某些条件下，在最小值附近可以实现超线性收敛速度。在不显式计算海森矩阵的情况下包含曲率信息。
- 缺点：需要可靠的梯度信息；在梯度有噪声的情况下，性能可能明显下降。由于需要存储和更新海森近似矩阵 $B_{k}$ （对于N个参数，大小为 $N\times N$ ）并执行线搜索（这可能需要多次函数/梯度评估），每次迭代的计算成本可能比SGD或Adam更高。可能不适用于维度非常高的参数空间。

## 2.2 无梯度优化器

这些优化器不需要显式计算梯度 $\nabla_{\theta} C(\theta)$ 。它们通常通过在参数空间中的不同点评估成本函数  $C(\theta)$ ，并使用这些信息来指导搜索。当梯度难以或昂贵计算时，或者当成本函数不可微分或噪声极大时，它们可能具有优势。

### SPSA（同步扰动随机近似）

SPSA特别适合于优化评估有噪声的函数，这在变分量子算法中是由于有限的测量统计数据而出现的常见情况。其最重要特点是，它仅使用两次成本函数评估来估算梯度方向，无论参数数量 $N$ 是多少。

在每次迭代 $k$ 中，SPSA按以下步骤进行：

1.  生成一个随机扰动向量 $\Delta_{k}$ ，其中每个分量通常独立地从简单分布（如Rademacher，即 $\pm 1$ ）中抽取。
2.  选择围绕当前参数向量 $\theta_{k}$ 对称扰动的两个点： $\theta_{k}^{+}=\theta_{k}+c_{k}\Delta_{k}$ 和 $\theta_{k}^{-}=\theta_{k}-c_{k}\Delta_{k}$ 。
3.  在这两个点评估成本函数: $C(\theta_{k}^{+})$ 和 $C(\theta_{k}^{-})$ 。
4.  分量地估算梯度： 
```math
(g_{k})_{i}\approx\frac{C(\theta_{k}^{+})-C(\theta_{k}^{-})}{2c_{k}(\Delta_{k})_{i}}
```
5.  更新参数： $\theta_{k+1}=\theta_{k}-a_{k} g_{k}$ 。
序列 $a_{k}$ （步长）和 $c_{k}$ （扰动大小）是预定义的递减序列，它们必须满足某些收敛条件。

*   **优点**：每次迭代仅需两次函数评估，当函数评估（电路执行）是主要成本时，这使其很高效。与依赖精确梯度估算的基于梯度的方法相比，对噪声相对稳定。不需要实现梯度计算逻辑（如参数位移）。
*   **缺点**：收敛速度可能比基于梯度的方法慢，特别是在高维或接近最小值时。性能可能对超参数 $a_{k}$ ， $c_{k}$ 的选择和扰动分布敏感。梯度估算本质上是随机的。

### 其他无梯度方法

有时会使用COBYLA(通过线性近似的约束优化)和Nelder-Mead等其他方法。COBYLA构建目标和约束的线性近似，而Nelder-Mead使用一个适应局部地形的单纯形(一种几何图形)。这些方法可能有用，但与SPSA相比，在参数数量较多或评估有噪声的情况下可能难以处理。

## 2.3 为您的变分量子算法选择优化器

优化器的最佳选择在很大程度上取决于具体的变分量子算法应用、PQC的特性、参数数量、可用的量子资源(模拟器对比硬件，噪声水平)，以及函数/梯度评估的成本。

<div align="center">

![Image](https://github.com/user-attachments/assets/a114c5c5-056f-4ed0-9f28-75e985459ad3)

*不同优化器在变分量子算法成本函数上的收敛行为示意性比较。Adam可能显示出快速的初始进展，而SPSA则表现出稳定且抗噪声的收敛。普通SGD通常收敛慢得多。实际性能因问题而异。*
</div>

*   **噪声敏感性**：如果在有噪声的硬件上运行或使用较少测量次数，SPSA等方法可能更可取。Adam有时可以处理中等噪声，而BFGS等方法通常更敏感。
*   **评估成本**：如果通过参数位移进行梯度计算(对于N个参数需要2N个额外电路)是可行的，并且梯度相对干净，那么Adam或其他基于梯度的方法可能收敛更快。如果电路评估是瓶颈，SPSA的每步两次评估方法则具有吸引力。
*   **参数数量**：对于参数数量非常多的情况，BFGS的内存需求可能变得难以承受。SPSA的每次迭代成本随参数数量的扩展性良好。
*   **问题结构**：对于可能表现出贫瘠高原(稍后讨论)的高度非凸曲面，不同优化器的搜索行为和噪声处理会明显影响它们能否找到好的解决方案或陷入困境。

# 3.量子自然梯度下降

虽然前一节讨论的先进经典优化器中的标准梯度下降方法为变分量子算法(VQA)的训练提供了起始点，但它们基于一个隐含的假设:参数空间 $\theta$ 是欧几里得的。在这种标准方法中，更新规则 $\theta^{(t+1)}=\theta^{(t)}-\eta\nabla L(\theta^{(t)})$ 在参数空间中沿最陡峭的下降方向移动参数。然而，我们优化的实际目标通常与参数化量子线路(PQC)产生的量子态 $|\psi(\theta)\rangle$ 有关，并且从参数 $\theta$ 到状态 $|\psi(\theta)\rangle$ 的映射可以是高度非线性和非均匀的。参数的微小变化可能导致参数空间某些区域的量子态发生显著变化，而参数的较大变化在其他地方可能只引起微小的状态改变。

这一观察促使我们采用考虑量子态空间本身几何结构的方法，而不再局限于标准梯度下降。正如第1章所介绍的，信息几何提供了分析统计模型结构的方法，包括我们的PQC产生的量子态。量子自然梯度(QNG)运用这种几何观点进行优化。

## 3.1 量子态的几何学与Fubini-Study度量

QNG背后的主要思想不是在平坦的参数流形上进行梯度下降，而是在参数 $\theta$ 所引起的量子态的弯曲流形上进行。两个无限接近的量子态 $|\psi(\theta)\rangle$ 和 $|\psi(\theta+d\theta)\rangle$ 之间的“距离”由[Fubini-Study度量](https://en.wikipedia.org/wiki/Fubini%E2%80%93Study_metric)衡量，在此背景下常被称为量子费雪信息矩阵(QFIM)。

令 $|\psi(\theta)\rangle$ 为由参数 $\theta=(\theta_{1},\ldots,\theta_{M})$ 的PQC制备的状态。Fubini-Study度量张量 $g_{ij}(\theta)$ 捕获了由参数变化 $d\theta_{i}$ 和 $d\theta_{j}$ 引起的状态之间平方的无穷小距离。其分量表示为:

```math
g_{ij}(\theta)=\text{Re}(\langle\partial_{i}\psi(\theta)|\partial_{j}\psi(\theta)\rangle-\langle\partial_{i}\psi(\theta)|\psi(\theta)\rangle\langle\psi(\theta)|\partial_{j}\psi(\theta)\rangle) 
```

式中 $|\partial_{i}\psi(\theta)\rangle=\frac{\partial|\psi(\theta)\rangle}{\partial\theta_{i}}$ 。这个度量张量形成一个 $M\times M$ 对称正半定矩阵 $G(\theta)$ ，它定量表示当我们改变参数时量子态在局部如何变化。它有效地说明了量子态对参数扰动的敏感度。

## 3.2 量子自然梯度更新规则

量子自然梯度通过预乘Fubini-Study度量张量 $G(\theta)^{-1}$ 的逆矩阵来修改标准梯度 $\nabla L(\theta)$ :
```math
\theta^{(t+1)}=\theta^{(t)}-\eta G(\theta^{(t)})^{-1}\nabla L(\theta^{(t)})
```
这里， $\eta$ 是学习率。这个更新规则直接在量子态流形上执行最陡下降步骤。通过 $G^{-1}$ 纳入几何信息，QNG更新步骤对PQC的具体参数化保持不变。它根据每个参数实际改变量子态的程度，有效地重新调整梯度分量，在参数对状态影响很小的方向上迈出更大的步长，而在参数高度敏感的方向上迈出更小的步长。

## 3.3 QNG的优点

1. 优化收敛性：通过根据状态空间的几何形状调整步长方向和大小，QNG与标准梯度下降相比，尤其是在参数到状态的映射高度非均匀时，通常能以更少的迭代次数收敛。

2. 参数化不变性：状态空间中自然梯度步长的方向与PQC的参数化方式无关，有助于实现更一致的优化行为。

3. 跨越平坦区域：在贫瘠高原区域，标准梯度会变得非常小，阻碍优化。尽管QNG并未从根本上解决贫瘠高原问题（该问题与梯度方差呈指数级消失有关），但QFIM中的几何信息有时可以帮助在标准梯度很小时，也能辨识状态空间中有意义的变化方向，可能有助于在某些优化环境中前进。

## 3.4 挑战与实际实施

使用QNG的主要挑战是Fubini-Study度量张量G(θ)的计算和求逆。

1. 计算成本：计算G(θ)的所有 $M^{2}$ 个分量通常需要O(M²)次期望值估计。有多种方法可以估计分量 $g_{ij}(\theta)$ ，通常涉及评估由轻微扰动参数产生的状态之间的重叠，或使用与线性响应理论相关的技术，有时可以利用参数平移规则等梯度计算方法。

2. 矩阵求逆：对M×M矩阵G(θ)求逆通常需要O(M³)的经典计算量，对于具有大量参数的PQC来说，这可能变得过于昂贵。

3. 奇异性与正则化：矩阵G(θ)有时可能是奇异的或接近奇异的，特别是当参数冗余或对状态影响很小时。实际应用中，在求逆前通常会添加一个正则化项： $(G(\theta)+\lambda I)^{-1}$ ，其中λ是一个小的正数（阻尼因子），I是单位矩阵。这可确保数值稳定性。

## 3.5 近似方法

由于计算开销，通常使用QFIM的近似方法：

- 块对角近似：如果参数可以分组（例如，按PQC中的层分组），可以将G(θ)近似为块对角矩阵，假设不同块中的参数之间几何关联性可以忽略不计。这大幅降低了求逆成本。

- 对角近似：一种更简单的方法是只计算并使用对角元素 $g_{ii}(\theta)$ ，将G(θ)视为对角矩阵。这相当于根据每个参数自身的敏感度单独重新调整其梯度更新，忽略参数之间的关联。计算成本更低但捕捉到的几何信息较少。

# 4.荒原高原

